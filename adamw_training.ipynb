{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfJwRnREsqDU"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5GSTkCUm9rx"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken\n",
        "!pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vj4sH3WUS3WV"
      },
      "outputs": [],
      "source": [
        "!wget https://cdn.jsdelivr.net/gh/karpathy/nanoGPT/model.py -O model.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7PRBBFoqGfH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tqdm\n",
        "from model import GPTConfig, GPT\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWRXl3qLxE9q"
      },
      "outputs": [],
      "source": [
        "import wandb, os\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik1-dz9EpIBw"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERZSAEOK7WG7"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data\n",
        "DATASET_FILE=\"data/tinystories_input.txt\"\n",
        "DATASET_URL=\"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt\"\n",
        "!wget $DATASET_URL -O $DATASET_FILE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVyXnyEL7a_J"
      },
      "outputs": [],
      "source": [
        "import os, requests, tiktoken, numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "input_file_path = \"data/tinystories.txt\"\n",
        "\n",
        "if not os.path.exists(input_file_path):\n",
        "    url = \"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt\"\n",
        "    with open(input_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(requests.get(url).text)\n",
        "\n",
        "num_lines = sum(1 for _ in open(input_file_path, \"r\"))\n",
        "split = int(0.9 * num_lines)\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "train_out = \"data/tinystories_train.bin\"\n",
        "val_out = \"data/tinystories_val.bin\"\n",
        "train_f = open(train_out, \"wb\")\n",
        "val_f = open(val_out, \"wb\")\n",
        "\n",
        "with open(input_file_path, \"r\") as f:\n",
        "    for i, line in enumerate(tqdm(f, total=num_lines)):\n",
        "        tokens = enc.encode(line, allowed_special={\"<|endoftext|>\"})\n",
        "        arr = np.array(tokens, dtype=np.uint16)\n",
        "        if i < split:\n",
        "            arr.tofile(train_f)\n",
        "        else:\n",
        "            arr.tofile(val_f)\n",
        "\n",
        "train_f.close(); val_f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ALo1lbquyb8"
      },
      "source": [
        "## AdamW Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_CjyeBi7n8B"
      },
      "outputs": [],
      "source": [
        "import os, time, torch, numpy as np, tqdm, wandb\n",
        "from model import GPT, GPTConfig\n",
        "\n",
        "LR_VALUES =  [3e-4, 1e-4, 1e-3, 3e-3, 1e-2]\n",
        "WD_VALUES =  [0, 1e-1, 1e-2, 1e-3, 1e-4]\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "      'name': 'val_loss',\n",
        "      'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'learning_rate': {\n",
        "            'values': LR_VALUES\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': WD_VALUES\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "train_data = np.memmap(\"data/tinystories_train.bin\", dtype=np.uint16, mode=\"r\")\n",
        "val_data = np.memmap(\"data/tinystories_val.bin\", dtype=np.uint16, mode=\"r\")\n",
        "vocab_size = 50304\n",
        "block_size = 256\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "\n",
        "model_config = GPTConfig(\n",
        "    block_size=block_size,\n",
        "    vocab_size=vocab_size,\n",
        "    n_layer=6,\n",
        "    n_head=6,\n",
        "    n_embd=384,\n",
        "    dropout=0.1,\n",
        "    bias=False\n",
        ")\n",
        "\n",
        "def get_batch(split, batch_size=24):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy(data[i:i+block_size].astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy(data[i+1:i+block_size+1].astype(np.int64)) for i in ix])\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, eval_iters=100):\n",
        "    model.eval()\n",
        "    losses = {}\n",
        "    for split in ['train', 'val']:\n",
        "        loss_sum = 0\n",
        "        for _ in range(eval_iters):\n",
        "            xb, yb = get_batch(split)\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            _, loss = model(xb, yb)\n",
        "            loss_sum += loss.item()\n",
        "        losses[split] = loss_sum / eval_iters\n",
        "    model.train()\n",
        "    return losses\n",
        "\n",
        "def train_one_run():\n",
        "    with wandb.init() as run:\n",
        "        config = wandb.config\n",
        "\n",
        "        run.name = f\"lr_{config.learning_rate:.0e}_wd_{config.weight_decay}\"\n",
        "        print(f\"--- Starting run: {run.name} ---\")\n",
        "\n",
        "        model = GPT(model_config).to(device)\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config.learning_rate,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "        start_time = time.time()\n",
        "        for iter_num in tqdm.tqdm(range(max_iters), desc=run.name):\n",
        "            xb, yb = get_batch('train')\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            logits, loss = model(xb, yb)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if iter_num % eval_interval == 0 or iter_num == max_iters - 1:\n",
        "                losses = estimate_loss(model)\n",
        "                elapsed = time.time() - start_time\n",
        "                current_val_loss = losses[\"val\"]\n",
        "\n",
        "                if current_val_loss < best_val_loss:\n",
        "                    best_val_loss = current_val_loss\n",
        "                    checkpoint_path = os.path.join(wandb.run.dir, \"best_model.pt\")\n",
        "                    torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "                wandb.log({\n",
        "                    \"iter\": iter_num,\n",
        "                    \"train_loss\": loss.item(),\n",
        "                    \"val_loss\": current_val_loss,\n",
        "                    \"best_val_loss\": best_val_loss,\n",
        "                    \"elapsed_time\": elapsed\n",
        "                })\n",
        "\n",
        "        ckpt_path = f\"checkpoint_iter_{iter_num}.pt\"\n",
        "        torch.save({\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"iter\": iter_num,\n",
        "            \"config\": dict(wandb.config),\n",
        "        }, ckpt_path)\n",
        "\n",
        "        artifact_name = f\"model-{run.name}-{run.id}\"\n",
        "        artifact = wandb.Artifact(\n",
        "            name=artifact_name,\n",
        "            type=\"model\"\n",
        "        )\n",
        "        artifact.add_file(ckpt_path)\n",
        "        run.log_artifact(artifact)\n",
        "\n",
        "# Create the sweep\n",
        "sweep_id = wandb.sweep(\n",
        "    sweep_config,\n",
        "    entity=\"182-proj\",\n",
        "    project=\"nano-gpt-adamw-200k\"\n",
        ")\n",
        "print(sweep_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "lRFLCY2y3spl",
        "outputId": "c56672d2-7968-4add-f4ff-e1ee3e0d0409"
      },
      "outputs": [],
      "source": [
        "wandb.agent(f\"182-proj/nano-gpt-adamw-200k/{sweep_id}\", function=train_one_run)\n",
        "print(\"Sweep complete.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
